# Awesome Reproducible Research [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3564746.svg)](https://doi.org/10.5281/zenodo.3564746)


<meta property="og:image" content="https://github.com/leipzig/awesome-reproducible-research/blob/master/rrlogo2.png?raw=true"/>
<img src="https://github.com/leipzig/awesome-reproducible-research/blob/master/rrlogo2tr.png?raw=true" align="right" width="200">

> A curated list of reproducible research case studies, projects, tutorials, and media


## Contents

- [Case studies](#case-studies)
- [Ad-hoc reproductions](#ad-hoc-reproductions)
- [Theory papers](#theory-papers)
- [Tool reviews](#tool-reviews)
- [Courses](#courses)
- [Development Resources](#development-resources)
- [Literature tools](#literature-tools)
- [Scientific Data Management Systems](#scientific-data-management-systems)
- [Books](#books)
- [Databases](#databases)
- [Data Repositories](#data-repositories)
- [Exemplar Portals](#exemplar-portals)
- [Runnable Papers](#runnable-papers)
- [Journals](#journals)
- [Ontologies](#ontologies)
- [Minimal Standards](#minimal-standards)
- [Organizations](#organizations)
- [Awesome Lists](#awesome-lists)

## Case studies
The term "case studies" is used here in a general sense to describe any study of reproducibility. A _reproduction_ is an attempt to arrive at comparable results with identical data using computational methods described in a paper. A _refactor_ involves refactoring existing code into frameworks and other reproducibility best practices while preserving the original data. A _replication_ involves generating new data and applying existing methods to achieve comparable results. A _robustness test_ applies various protocols, workflows, statistical models or parameters to a given data set to study their effect on results, either as a follow-up to an existing study or as a "bake-off". A _census_ is a high-level tabulation conducted by a third party. A _survey_ is a questionnaire sent to practitioners. A _case narrative_ is an in-depth first-person account. An _independent discussion_ utilizes a secondary independent author to interpret the results of a study as a means to improve inferential reproducibility.
<table id="case_studies">
			<tbody>
				<tr>
					<td>
						<p>
							Study <meta property="embeddedDate">
						</p>
					</td>
					<td>
						<p>
							Field
						</p>
					</td>
					<td>
						<p>
							Approach
						</p>
					</td>
					<td>
						<p>
							Size
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1136/bmj.39590.732037.47"><span title="What is missing from descriptions of treatment in trials and reviews?">Glasziou et al <meta property="datePublished" content="2008-06-26">2008</span></a>
						</p>
					</td>
					<td>
						<p>
							Medicine
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							80 studies
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1214/09-AOAS291"><span title="DERIVING CHEMOSENSITIVITY FROM CELL LINES: FORENSIC BIOINFORMATICS AND REPRODUCIBLE RESEARCH IN HIGH-THROUGHPUT BIOLOGY">Baggerly &amp; Coombes <meta property="datePublished" content="2009-09-01">2009</span></a>
						</p>
					</td>
					<td>
						<p>
							Cancer biology
						</p>
					</td>
					<td>
						<p>
							Refactor
						</p>
					</td>
					<td>
						<p>
							8 studies
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1002/bimj.200900154"><span title="Biometrical Journal and Reproducible Research">Hothorn et al. <meta property="datePublished" content="2009-08-17">2009</span></a>
						</p>
					</td>
					<td>
						<p>
							Biostatistics
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							56 studies
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1038/ng.295"><span title="Repeatability of published microarray gene expression analyses">Ioannidis et al <meta property="datePublished" content="2009-01-28">2009</span></a>
						</p>
					</td>
					<td>
						<p>
							Genetics
						</p>
					</td>
					<td>
						<p>
							Reproduction
						</p>
					</td>
					<td>
						<p>
							18 studies
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="http://www.uio.no/studier/emner/matnat/ifi/INF5700/h11/undervisningsmateriale/Anda.Sj%C3%B8berg.Mockus.TSE.May.2009.pdf">Anda et al <meta property="datePublished" content="2009-07-30">2009</a>
						</p>
					</td>
					<td>
						<p>
							Software engineering
						</p>
					</td>
					<td>
						<p>
							Replication
						</p>
					</td>
					<td>
						<p>
							4 companies
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://infoscience.epfl.ch/record/136640/files/VandewalleKV09.pdf">Vandewalle et al <meta property="datePublished" content="2009-04-22">2009</a>
						</p>
					</td>
					<td>
						<p>
							Signal processing
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							134 papers
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1038/nrd3439-c1">Prinz <meta property="datePublished" content="2011-08-31">2011</a>
						</p>
					</td>
					<td>
						<p>
							Biomedical sciences
						</p>
					</td>
					<td>
						<p>
							Survey
						</p>
					</td>
					<td>
						<p>
							23 PIs
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://academic.oup.com/bib/article/12/3/288/258098/Case-studies-in-reproducibility">Horthorn &amp; Leisch <meta property="datePublished" content="2011-01-28">2011</a>
						</p>
					</td>
					<td>
						<p>
							Bioinformatics
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							100 studies
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://www.nature.com/nature/journal/v483/n7391/full/483531a.html">Begley &amp; Ellis <meta property="datePublished" content="2012-03-29">2012</a>
						</p>
					</td>
					<td>
						<p>
							Cancer biology
						</p>
					</td>
					<td>
						<p>
							Replication
						</p>
					</td>
					<td>
						<p>
							53 studies
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="http://reproducibility.cs.arizona.edu/tr.pdf">Collberg et al <meta property="datePublished" content="2014-03-21">2014</a><br/><a href="https://sci-hub.se/10.1145/2812803">Collberg &amp; Proebsting 2016</a>
						</p>
					</td>
					<td>
						<p>
							Computer science
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							613 papers
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="http://science.sciencemag.org/content/349/6251/aac4716">OSC <meta property="datePublished" content="2015-08-28">2015</a>
						</p>
					</td>
					<td>
						<p>
							Psychology
						</p>
					</td>
					<td>
						<p>
							Replication
						</p>
					</td>
					<td>
						<p>
							100 studies
						</p>
					</td>
				</tr>
				<tr>
					<td>
					<p>
					<a href="https://f1000research.com/articles/4-134/v2">Bandrowski et al <meta property="datePublished" content="2015-05-29">2015</a>
					</p>
					</td>
					<td>
						<p>
							Biomedical sciences
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							100 papers
						</p>
					</td>
				</tr>
								<tr>
					<td>
					<p>
					<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4555355/">Patel et al <meta property="datePublished" content="2015-06-06">2015</a>
					</p>
					</td>
					<td>
						<p>
							Epidemiology
						</p>
					</td>
					<td>
						<p>
							Robustness test
						</p>
					</td>
					<td>
						<p>
							417 variables
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.17016/FEDS.2015.083">Chang et al
							<meta property="datePublished" content="2015-10-05">2015</a>
						</p>
					</td>
					<td>
						<p>
							<span title="Is Economics Research Replicable? Sixty Published Papers from Thirteen Journals Say &quot;Usually Not&quot;">Economics</span>
						</p>
					</td>
					<td>
						<p>
							Reproduction
						</p>
					</td>
					<td>
						<p>
							67 papers
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1371/journal.pbio.1002333">Iqbal et al
							<meta property="datePublished" content="2016-01-04">2016</a>
						</p>
					</td>
					<td>
						<p>
							<span title="Reproducible Research Practices and Transparency across the Biomedical Literature">Biomedical sciences</span>
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							441 papers
						</p>
					</td>
				</tr>
				<tr>
					<td>
					<p>
					<a href="https://doi.org/10.1038/533452a"><span title="1,500 scientists lift the lid on reproducibility">Baker <meta property="datePublished" content="2016-05-26">2016</span></a>
					</p>
					</td>
					<td>
						<p>
							Science
						</p>
					</td>
					<td>
						<p>
							<span title="More than 70% of researchers have tried and failed to reproduce another scientist's experiments, and more than half have failed to reproduce their own experiments. Those are some of the telling figures that emerged from Nature's survey of 1,576 researchers who took a brief online questionnaire on reproducibility in research.">Survey</span>
						</p>
					</td>
					<td>
						<p>
							1,576 researchers
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://pdfs.semanticscholar.org/edd7/e68711955cbbdb6dd6866db2ec8a6ff18278.pdf">Névéol et al <meta property="datePublished" content="2016-11-05">2016</a>
						</p>
					</td>
					<td>
						<p>
							NLP
						</p>
					</td>
					<td>
						<p>
							Replication
						</p>
					</td>
					<td>
						<p>
							3 studies
						</p>
					</td>
				</tr>
								<tr>
					<td>
						<p>
							<a href="https://elifesciences.org/articles/23383#abstract">Reproducibility Project <meta property="datePublished" content="2017-01-19">2017</a>
						</p>
					</td>
					<td>
						<p>
							Cancer biology
						</p>
					</td>
					<td>
						<p>
							Replication
						</p>
					</td>
					<td>
						<p>
							9 studies
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
						<a href="https://peerj.com/articles/3208/">Vasilevsky et al <meta property="datePublished" content="2017-04-25">2017</a>
						</p>
					</td>
					<td>
						<p>
							Biomedical sciences
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							318 journals
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
						<a href="http://www.practicereproducibleresearch.org/">Kitzes et al <meta property="datePublished" content="2017-10-17">2017</a>
						</p>
					</td>
					<td>
						<p>
							Science
						</p>
					</td>
					<td>
						<p>
							Case narrative
						</p>
					</td>
					<td>
						<p>
							31 PIs
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005755">Barone et al <meta property="datePublished" content="2017-10-19">2017</a>
						</p>
					</td>
					<td>
						<p>
						        Biological sciences
						<p>
					</td>
					<td>
						<p>
							Survey
						</p>
					</td>
					<td>
						<p>
							704 PIs
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://www.biorxiv.org/content/biorxiv/early/2017/10/31/143503.full.pdf?%3Fcollection=">Kim &amp; Dumas <meta property="datePublished" content="2017-10-31">2017</a>
						</p>
					</td>
					<td>
						<p>
							Bioinformatics
						</p>
					</td>
					<td>
						<p>
							Refactor
						</p>
					</td>
					<td>
						<p>
							1 study
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="http://science.sciencemag.org/content/351/6280/1433">Camerer <meta property="datePublished" content="2016-03-25">2017</a>
						</p>
					</td>
					<td>
						<p>
							Economics
						</p>
					</td>
					<td>
						<p>
							Replication
						</p>
					</td>
					<td>
						<p>
							18 studies
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://openreview.net/pdf?id=By4l2PbQ-">Olorisade <meta property="datePublished" content="2017-08-06">2017</a>
						</p>
					</td>
					<td>
						<p>
							Machine learning
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							30 studies
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1515/opar-2017-0019">Strupler &amp; Wilkinson <meta property="datePublished" content="2017-11-14">2017</a>
						</p>
					</td>
					<td>
						<p>
							Archaeology
						</p>
					</td>
					<td>
						<p>
							Case narrative
						</p>
					</td>
					<td>
						<p>
							1 survey
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://arxiv.org/abs/1801.05042">Danchev et al <meta property="datePublished" content="2018-01-15">2017</a>
						</p>
					</td>
					<td>
						<p>
							Comparative toxicogenomics
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							51,292 claims in 3,363 papers
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://www.researchgate.net/publication/326450530_State_of_the_Art_Reproducibility_in_Artificial_Intelligence">Kjensmo &amp; Gundersen <meta property="datePublished" content="2018-04-25">2018</a>
						</p>
					</td>
					<td>
						<p>
							Artificial intelligence
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							400 papers
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://www.nature.com/articles/d41586-018-02108-9">Gertler et al <meta property="datePublished" content="2018-02-21">2018</a>
						</p>
					</td>
					<td>
						<p>
							Economics
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							203 papers
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="http://www.pnas.org/content/115/11/2584">Stodden et al <meta property="datePublished" content="2018-03-13">2018</a>
						</p>
					</td>
					<td>
						<p>
							Computational science
						</p>
					</td>
					<td>
						<p>
							Reproduction
						</p>
					</td>
					<td>
						<p>
							204 papers, 180 authors
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="http://doi.org/10.1371/journal.pone.0213013">Madduri et al <meta property="datePublished" content="2019-04-09">2018</a>
						</p>
					</td>
					<td>
						<p>
							Genomics
						</p>
					</td>
					<td>
						<p>
							Case narrative
						</p>
					</td>
					<td>
						<p>
							1 study
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1038/s41562-018-0399-z">Camerer et al <meta property="datePublished" content="2018-08-27">2018</a>
						</p>
					</td>
					<td>
						<p>
							Social sciences
						</p>
					</td>
					<td>
						<p>
							Replication
						</p>
					</td>
					<td>
						<p>
							21 papers
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="http://doi.org/10.1177/2515245917747646">Silberzahn et al <meta property="datePublished" content="2018-08-23">2018</a>
						</p>
					</td>
					<td>
						<p>
							Psychology
						</p>
					</td>
					<td>
						<p>
							Robustness test
						</p>
					</td>
					<td>
						<p>
							One data set, 29 analyst teams
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1002/bimj.201700243">Boulesteix et al <meta property="datePublished" content="2018-08-01">2018</a>
						</p>
					</td>
					<td>
						<p>
							Medicine and health sciences
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							30 papers
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.7554/eLife.34364">Eaton et al <meta property="datePublished" content="2018-10-03">2018</a>
						</p>
					</td>
					<td>
						<p>
							Microbiome immuno oncology
						</p>
					</td>
					<td>
						<p>
							Replication
						</p>
					</td>
					<td>
						<p>
							1 paper
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://www.biorxiv.org/content/early/2018/11/08/463927">Vaquero-Garcia et al <meta property="datePublished" content="2018-11-08">2018</a>
						</p>
					</td>
					<td>
						<p>
							Bioinformatics
						</p>
					</td>
					<td>
						<p>
							Refactor and test of robustness
						</p>
					</td>
					<td>
						<p>
							1 paper
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1371/journal.pbio.2006930">Wallach et al <meta property="datePublished" content="2018-11-20">2018</a>
						</p>
					</td>
					<td>
						<p>
							Biomedical Sciences
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							149 papers
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://broad.io/ASHG2018">Miller et al <meta property="datePublished" content="2018-10-18">2018</a>
						</p>
					</td>
					<td>
						<p>
							Bioinformatics
						</p>
					</td>
					<td>
						<p>
							Synthetic replication & refactor
						</p>
					</td>
					<td>
						<p>
							1 paper
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1080/13658816.2018.1508687">Konkol et al <meta property="datePublished" content="2018-04-09">2018</a>
						</p>
					</td>
					<td>
						<p>
							Geosciences
						</p>
					</td>
					<td>
						<p>
							Survey, Reproduction
						</p>
					</td>
					<td>
						<p>
							146 scientists, 41 papers
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="http://amid.fish/reproducing-deep-rl">Rahtz <meta property="datePublished" content="2018-04-06">2018<a>
								</p>
								</td>
							<td>
								<p>
									Reinforcement Learning
								</p>
							</td>
							<td>
								<p>
									Reproduction, case narrative
								</p>
							<td>
								<p>
									1 paper
								</p>
							</td>
						</p>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1145/3214239.3214242">Stodden et al
							<meta property="datePublished" content="2018-06-12">2018</a>
						</p>
					</td>
					<td>
						<p>
							<span title="Enabling the Verification of Computational Results">Computational physics</span>
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							306 papers
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://peerj.com/articles/cs-163/">AlNoamany & Borghi <meta property="datePublished" content="2018-09-17">2018</a>
						</p>
					</td>
					<td>
						<p>
							Science & Engineering
						</p>
					</td>
					<td>
						<p>
							Survey
						</p>
					</td>
					<td>
						<p>
							215 participants
						</p>
					</td>
				</tr>
								<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1155/2018/4789035
">Li et al <meta property="datePublished" content="2018-09-27">2018</a>
						</p>
					</td>
					<td>
						<p>
							Nephrology
						</p>
					</td>
					<td>
						<p>
							Robustness test
						</p>
					</td>
					<td>
						<p>
							1 paper
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p><a href="https://dash.harvard.edu/bitstream/handle/1/38811561/CHEN-SENIORTHESIS-2018.pdf?sequence=3
">Chen <meta property="datePublished" content="2018-06-29">2018</a>
						</p>
					</td>
					<td>
						<p>
							Social sciences & other
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							810 Dataverse studies
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://arxiv.org/abs/2103.12793">Trisovic et al
							<meta property="datePublished" content="2021-03-23">2021</a>
						</p>
					</td>
					<td>
						<p>
							<span title="A large-scale study on research code quality and execution">Social sciences & other</span>
						</p>
					</td>
					<td>
						<p>
							Census, Survey
						</p>
					</td>
					<td>
						<p>
							2109 replication datasets
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.7717/peerj.5072">Nüst et al
							<meta property="datePublished" content="2018-07-13">2018</a>
						</p>
					</td>
					<td>
						<p>
							<span title="Reproducible research and GIScience: an evaluation using AGILE conference papers">GIScience/Geoinformatics</span>
						</p>
					</td>
					<td>
						<p>
							Census, Survey
						</p>
					</td>
					<td>
						<p>
							32 papers, 22 participants
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1038/s41467-018-05627-1">Raman et al
							<meta property="datePublished" content="2018-08-07">2018</a>
						</p>
					</td>
					<td>
						<p>
							<span title="Apparent bias toward long gene misregulation in MeCP2 syndromes disappears after controlling for baseline variations">Genomics</span>
						</p>
					</td>
					<td>
						<p>
							Robustness test
						</p>
					</td>
					<td>
						<p>
							8 studies
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1038/sdata.2019.30">Stagge et al <meta property="datePublished" content="2019-02-26">2019</a>
						</p>
					</td>
					<td>
						<p>
							Geosciences
						</p>
					</td>
					<td>
						<p>
							Survey
						</p>
					</td>
					<td>
						<p>
							360 papers
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1371/journal.pcbi.1006269">Bizzego et al <meta property="datePublished" content="2019-03-27">2019</a>
						</p>
					</td>
					<td>
						<p>
							Deep learning
						</p>
					</td>
					<td>
						<p>
							Robustness test
						</p>
					</td>
					<td>
						<p>
							1 analysis
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1371/journal.pone.0213013">Madduri et al <meta property="datePublished" content="2019-04-11">2019</a>
						</p>
					</td>
					<td>
						<p>
							Genomics
						</p>
					</td>
					<td>
						<p>
							Case narrative
						</p>
					</td>
					<td>
						<p>
							1 analysis
						</p>
					</td>
				</tr>
								<tr>
					<td>
						<p>
							<span title="Creating reproducible pharmacogenomic analysis pipelines"><a href="10.1038/s41597-019-0174-7">Mammoliti
								et al <meta property="datePublished" content="2019-09-03">2019</a></span>
						</p>
					</td>
					<td>
						<p>
							Pharmacogenomics
						</p>
					</td>
					<td>
						<p>
							Case narrative
						</p>
					</td>
					<td>
						<p>
							2 analyses
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p><a href="https://doi.org/10.1371/journal.pbio.3000246"><span title="Open science challenges, benefits and tips in early career and beyond">Allen & Mehler <meta property="datePublished" content="2019-05-01">2019</span></a>
						</p>
					</td>
					<td>
						<p>
							Biomedical sciences and Psychology
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							127 registered reports 
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p><a href="https://doi.org/10.1109/MSR.2019.00077"><span title="A Large-scale Study about Quality and
Reproducibility of Jupyter Notebooks">Pimentel et al <meta property="datePublished" content="2019-05-07">2019</span></a>
						</p>
					</td>
					<td>
						<p>
							All
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							 1,159,166 Jupyter notebooks 
						</p>
					</td>
				</tr>
								<tr>
					<td>
						<p><a href="https://doi.org/10.1016/j.omto.2019.05.004"><span title="Assessing the Completeness of Reporting in Preclinical Oncolytic Virus Therapy Studies">Fergusson et al <meta property="datePublished" content="2019-05-20">2019</span></a>
						</p>
					</td>
					<td>
						<p>
							Virology
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							 236 papers 
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p><a href="https://doi.org/10.1016/j.bja.2019.01.012
"><span title="Hypnotic depth and postoperative death: a Bayesian perspective and an Independent Discussion of a clinical trial">Vlisides et al <meta property="datePublished" content="2019-01-22">2019</span></a><br/><a href="https://doi.org/10.1016/j.bja.2018.12.021"><span title="Depth of sedation as an interventional target to reduce postoperative delirium: mortality and functional outcomes of the Strategy to Reduce the Incidence of Postoperative Delirium in Elderly Patients randomised clinical trial">Sieber et al 2019</span></a>
						</p>
					</td>
					<td>
						<p>
							Anaesthesia
						</p>
					</td>
					<td>
						<p>
							Independent discussion
						</p>
					</td>
					<td>
						<p>
							 1 study 
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p><a href="https://osf.io/7yt8u/
"><span title="Replication Oxley et al. (2008, Science)">Bakker et al <meta property="datePublished" content="2019-06-19">2019</span></a>
						</p>
					</td>
					<td>
						<p>
							Psychology
						</p>
					</td>
					<td>
						<p>
							Replication
						</p>
					</td>
					<td>
						<p>
							 1 paper 
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p><a href="https://doi.org/10.1016/j.cels.2019.06.005
"><span title="A Multi-center Study on the Reproducibility of Drug-Response Assays in Mammalian Cell Lines">Niepel et al <meta property="datePublished" content="2019-07-10">2019</span></a>
						</p>
					</td>
					<td>
						<p>
							Cell pharmacology
						</p>
					</td>
					<td>
						<p>
							Robustness test
						</p>
					</td>
					<td>
						<p>
							 5 labs 
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p><a href="https://arxiv.org/abs/1907.06902v1"><span title="
Are We Really Making Much Progress? A Worrying Analysis of Recent Neural Recommendation Approaches">Dacrema et al <meta property="datePublished" content="2019-07-16">2019</span></a>
						</p>
					</td>
					<td>
						<p>
							Machine learning
						</p>
					</td>
					<td>
						<p>
							Reproduction
						</p>
					</td>
					<td>
						<p>
							18 conference papers 
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p><a href="https://doi.org/10.1016/j.jasrep.2019.102002"><span title="Experimental replication shows knives manufactured from frozen human feces do not work">Eran et al <meta property="datePublished" content="2019-10-09">2019</span></a>
						</p>
					</td>
					<td>
						<p>
							Experimental archaeology
						</p>
					</td>
					<td>
						<p>
							Replication
						</p>
					</td>
					<td>
						<p>
							1 theory
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p><a href="https://doi.org/10.1101/763730"><span title="Reproducible and Transparent Research Practices in Published Neurology Research">Rauh et al <meta property="datePublished" content="2019-09-16">2019</span></a>
						</p>
					</td>
					<td>
						<p>
							Neurology
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							202 papers
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p><a href="https://doi.org/10.31234/osf.io/dkg53"><span title="Failed pre-registered replication of mortality salience effects in traditional and novel measures">Sætrevik & Sjåstad <meta property="datePublished" content="2019-09-20">2019</span></a>
						</p>
					</td>
					<td>
						<p>
							Psychology
						</p>
					</td>
					<td>
						<p>
							Replication
						</p>
					</td>
					<td>
						<p>
							2 experiments
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p><a href="https://doi.org/10.1038/s41559-019-0972-5"><span title="A checklist for maximizing reproducibility of ecological niche models">Feng et al. <meta property="datePublished" content="2019-09-23">2019</span></a>
						</p>
					</td>
					<td>
						<p>
							Ecology and Evolution
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							163 papers
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p><a href="https://doi.org/10.1101/843193"><span title="Variability in the analysis of a single neuroimaging dataset by many teams">Botvinik-Nezer et al. <meta property="datePublished" content="2019-11-15">2019</span></a>
						</p>
					</td>
					<td>
						<p>
							Neuroimaging
						</p>
					</td>
					<td>
						<p>
							Robustness test
						</p>
					</td>
					<td>
						<p>
							1 data set, 70 teams
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p><a href="https://doi.org/10.31234/osf.io/vef2c"><span title="Many Labs 4: Failure to Replicate Mortality Salience Effect With and Without Original Author Involvement">Klein et al. <meta property="datePublished" content="2019-12-11">2019</span></a>
						</p>
					</td>
					<td>
						<p>
							Psychology
						</p>
					</td>
					<td>
						<p>
							<span title="Many Labs 4: Interpreting a failure to replicate is complicated by the fact that the failure could be due to the original finding being a false positive, unrecognized moderating influences between the original and replication procedures, or faulty implementation of the procedures in the replication. One strategy to maximize replication quality is involving the original authors in study design. We (N = 21 Labs and N = 2,220 participants) experimentally tested whether original author involvement improved replicability of a classic finding from Terror Management Theory (Greenberg et al., 1994). Our results were non-diagnostic of whether original author involvement improves replicability because we were unable to replicate the finding under any conditions. This suggests that the original finding was either a false positive or the conditions necessary to obtain it are not yet understood or no longer exist.">Replication</span>
						</p>
					</td>
					<td>
						<p>
							1 experiment, 21 labs, 2,220 participants
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p><a href="https://doi.org/10.31234/osf.io/fk8vh"><span title="Analysis of Open Data and Computational Reproducibility in Registered Reports in Psychology">Obels et al. <meta property="datePublished" content="2019-05-23">2019</span></a>
						</p>
					</td>
					<td>
						<p>
							Psychology
						</p>
					</td>
					<td>
						<p>
							<span title="Ongoing technological developments have made it easier than ever before for scientists to share their data, materials, and analysis code. Sharing data and analysis code makes it easier for other researchers to re-use or check published research. However, these benefits will only emerge if researchers can reproduce the analysis reported in published articles and if data is annotated well enough so that it is clear what all variables mean. Because most researchers are not trained in computational reproducibility, it is important to evaluate current practices to identify practices that can be improved. We examined data and code sharing for Registered Reports published in the psychological literature between 2014 and 2018, and attempted to independently computationally reproduce the main results in each article. Of the main results from 62 articles that met our inclusion criteria, data were available for 41 articles, and analysis scripts for 37 articles. For the main results in 36 articles that shared both data and code we could run the scripts for 31 analyses, and reproduce the main results for 21 articles. Although the articles that shared both data and code (36 out of 62, or 58%) and articles for which main results could be computationally reproduced (21 out of 36, or 58%) was relatively high compared to other studies, there is clear room for improvement. We provide practical recommendations based on our observations and link to examples of good research practices in the papers we reproduced.">Census</span>
						</p>
					</td>
					<td>
						<p>
							62 papers
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1001/jamaoncol.2019.2564">Wayant et al
							<meta property="datePublished" content="2019-09-05">2019</a>
						</p>
					</td>
					<td>
						<p>
							<span title="Evaluation of Reproducible Research Practices in Oncology Systematic Reviews With Meta-analyses Referenced by National Comprehensive Cancer Network Guidelines">Oncology</span>
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							154 meta-analyses
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p><a href="https://doi.org/10.1101/2020.01.30.924092"><span title="Factorial study of the RNA-seq computational workflow identifies biases as technical gene signatures">Simoneau et al. <meta property="datePublished" content="2020-01-30">2020</span></a>
						</p>
					</td>
					<td>
						<p>
							Bioinformatics
						</p>
					</td>
					<td>
						<p>
							<span title="RNA-seq is a modular experimental and computational approach that aims in identifying and quantifying RNA molecules. The modularity of the RNA-seq technology enables adaptation of the protocol to develop new ways to explore RNA biology, but this modularity also brings forth the importance of methodological thoroughness. Liberty of approach comes with the responsibility of choices, and such choices must be informed. Here, we present an approach that identifies gene group specific quantification biases in currently used RNA-seq software and references by processing sequenced datasets using a wide variety of RNA-seq computational pipelined, and by decomposing these expression datasets using an independent component analysis matrix factorisation method. By exploring the RNA-seq pipeline using a systemic approach, we highlight the yet inadequately characterized central importance of genome annotations in quantification results. We also show that the different choices in RNA-seq methodology are not independent, through interactions between genome annotations and quantification software. Genes were mainly found to be affected by differences in their sequence, by overlapping genes and genes with similar sequence. Our approach offers an explanation for the observed biases by identifying the common features used differently by the software and references, therefore providing leads for the betterment of RNA-seq methodology.">Robustness test</span>
						</p>
					</td>
					<td>
						<p>
							1 data set
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p><a href="https://doi.org/10.1186/s13041-020-0552-2"><span title="No raw data, no science: another possible source of the reproducibility crisis">Miyakawa <meta property="datePublished" content="2020-02-20">2020</span></a>
						</p>
					</td>
					<td>
						<p>
							Neurobiology
						</p>
					</td>
					<td>
						<p>
							<span title="A reproducibility crisis is a situation where many scientific studies cannot be reproduced. Inappropriate practices of science, such as HARKing, p-hacking, and selective reporting of positive results, have been suggested as causes of irreproducibility. In this editorial, I propose that a lack of raw data or data fabrication is another possible cause of irreproducibility. As an Editor-in-Chief of Molecular Brain, I have handled 180 manuscripts since early 2017 and have made 41 editorial decisions categorized as Revise before review, requesting that the authors provide raw data. Surprisingly, among those 41 manuscripts, 21 were withdrawn without providing raw data, indicating that requiring raw data drove away more than half of the manuscripts. I rejected 19 out of the remaining 20 manuscripts because of insufficient raw data. Thus, more than 97% of the 41 manuscripts did not present the raw data supporting their results when requested by an editor, suggesting a possibility that the raw data did not exist from the beginning, at least in some portions of these cases. Considering that any scientific study should be based on raw data, and that data storage space should no longer be a challenge, journals, in principle, should try to have their authors publicize raw data in a public database or journal site upon the publication of the paper to increase reproducibility of the published results and to increase public trust in science.">Census</span>
						</p>
					</td>
					<td>
						<p>
							41 papers
						</p>
					</td>
				</tr><tr>
					<td>
						<p>
							<a href="https://doi.org/10.1371/journal.pone.0229578">Thelwall et al
							<meta property="datePublished" content="2020-02-21">2020</a>
						</p>
					</td>
					<td>
						<p>
							<span title="Is useful research data usually shared? An investigation of genome-wide association study summary statistics">Genetics</span>
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							1799 papers
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1371/journal.pone.0233107">Maassen et al
							<meta property="datePublished" content="2020-05-27">2020</a>
						</p>
					</td>
					<td>
						<p>
							<span title="Reproducibility of individual effect sizes in meta-analyses in psychology">Psychology</span>
						</p>
					</td>
					<td>
						<p>
							Reproduction
						</p>
					</td>
					<td>
						<p>
							33 meta-analyses
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1101/2020.05.11.088021">Riedel et al
							<meta property="datePublished" content="2020-05-13">2020</a>
						</p>
					</td>
					<td>
						<p>
							<span title="ODDPub – a Text-Mining Algorithm to Detect Data Sharing in Biomedical Publications">Biomedical science</span>
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							792 papers
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1371/journal.pbio.3000763">Culina et al
							<meta property="datePublished" content="2020-07-28">2020</a>
						</p>
					</td>
					<td>
						<p>
							<span title="Low availability of code in ecology: A call for urgent action">Ecology</span>
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							346 papers
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://arxiv.org/abs/2008.05414">Clementi & Barba
							<meta property="datePublished" content="2020-08-12">2020</a>
						</p>
					</td>
					<td>
						<p>
							<span title="Reproducible Validation and Replication Studies in Nanoscale Physics">Physics</span>
						</p>
					</td>
					<td>
						<p>
							Replication
						</p>
					</td>
					<td>
						<p>
							2 papers
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1016/j.fertnstert.2020.05.020">Kemper et al
							<meta property="datePublished" content="2020-08-06">2020</a>
						</p>
					</td>
					<td>
						<p>
							<span title="Reproducible research practices and transparency in reproductive endocrinology and infertility articles">Reproductive endocrinology</span>
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							222 papers
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.7554/eLife.55133">Marqués et al
							<meta property="datePublished" content="2020-08-11">2020</a>
						</p>
					</td>
					<td>
						<p>
							<span title="Imaging methods are vastly underreported in biomedical research">Biomedical sciences</span>
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							240 papers
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1016/j.envsoft.2020.104873">Janssen et al
							<meta property="datePublished" content="2020-09-16">2020</a>
						</p>
					</td>
					<td>
						<p>
							<span title="On code sharing and model documentation of published individual and agent-based models">Environmental Modelling</span>
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							7500 papers
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1136/heartjnl-2020-316519">Anderson et al
							<meta property="datePublished" content="2020-08-21">2020</a>
						</p>
					</td>
					<td>
						<p>
							<span title="Evaluation of indicators supporting reproducibility and transparency within cardiology literature">Cardiology</span>
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							532 papers
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.31223/x5zk5v">Ostermann et al
							<meta property="datePublished" content="2020-10-29">2020</a>
						</p>
					</td>
					<td>
						<p>
							<span title="Reproducible Research and GIScience: an evaluation using GIScience conference papers">GIS</span>
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							75 papers
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1101/581033">Samota & Davey
							<meta property="datePublished" content="2020-12-09">2020</a>
						</p>
					</td>
					<td>
						<p>
							<span title="Knowledge and attitudes among life scientists towards reproducibility within journal articles: a research survey">Life Sciences</span>
						</p>
					</td>
					<td>
						<p>
							Survey
						</p>
					</td>
					<td>
						<p>
							251 researchers
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1101/2020.10.27.20220939">Bedford & Tzovaras
							<meta property="datePublished" content="2020-10-29">2020</a>
						</p>
					</td>
					<td>
						<p>
							<span title="Re-analysis of genetic risks for Chronic Fatigue Syndrome from 23andMe data finds few remain">Genetics</span>
						</p>
					</td>
					<td>
						<p>
							Robustness test
						</p>
					</td>
					<td>
						<p>
							1 paper
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.3389/fgene.2020.610798">Krassowski et al
							<meta property="datePublished" content="2020-12-10">2020</a> <a href="https://github.com/krassowski/multi-omics-state-of-the-field">(repo)</a>
						</p>
					</td>
					<td>
						<p>
							<span title="State of the Field in Multi-Omics Research: From Computational Needs to Data Mining and Sharing">Life Sciences</span>
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							3377 articles
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1371/journal.pcbi.1008725">Boudreau et al
							<meta property="datePublished" content="2021-02-11">2021</a>
						</p>
					</td>
					<td>
						<p>
							<span title="On the open-source landscape of PLOS Computational Biology">Computational Biology</span>
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							622 papers
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://arxiv.org/abs/2104.07651">Heumos et al
							<meta property="datePublished" content="2021-04-15">2021</a>
						</p>
					</td>
					<td>
						<p>
							<span title="mlf-core: a framework for deterministic machine learning">Computational Biology</span>
						</p>
					</td>
					<td>
						<p>
							Robustness test
						</p>
					</td>
					<td>
						<p>
							6 studies
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.31219/osf.io/f73a6">Hrynaszkiewicz et al
							<meta property="datePublished" content="2021-04-28">2021</a>
						</p>
					</td>
					<td>
						<p>
							<span title="A survey of code sharing practice and policy in computational biology">Computational Biology</span>
						</p>
					</td>
					<td>
						<p>
							Survey
						</p>
					</td>
					<td>
						<p>
							214 researchers
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1101/2021.01.04.424681">Päll et al
							<meta property="datePublished" content="2021-01-04">2021</a>
						</p>
					</td>
					<td>
						<p>
							<span title="A field-wide assessment of differential high throughput sequencing reveals widespread bias">Differential expression</span>
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							2109 GEO submissions
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1101/2021.09.06.459114">Wijesooriya et al
							<meta property="datePublished" content="2021-09-08">2021</a>
						</p>
					</td>
					<td>
						<p>
							<span title="Guidelines for reliable and reproducible functional enrichment analysis">Computational biology</span>
						</p>
					</td>
					<td>
						<p>
							Census
						</p>
					</td>
					<td>
						<p>
							186 papers
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.31234/osf.io/c8a5z">Weisberg et al
							<meta property="datePublished" content="2021-09-06">2021</a>
						</p>
					</td>
					<td>
						<p>
							<span title="Evaluating the Effects of a Programming Error on a Virtual Environment Measure of Spatial Navigation Behavior">Psychology</span>
						</p>
					</td>
					<td>
						<p>
							Robustness test
						</p>
					</td>
					<td>
						<p>
							1 study
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1101/2021.04.12.439408">Vanderaa & Gatto
							<meta property="datePublished" content="2021-04-12">2021</a>
						</p>
					</td>
					<td>
						<p>
							<span title="Replication of single-cell proteomics data reveals important computational challenges">Proteomics</span>
						</p>
					</td>
					<td>
						<p>
							Refactor
						</p>
					</td>
					<td>
						<p>
							1 analysis
						</p>
					</td>
				</tr>
				<!--study_placeholder-->
			</tbody>
		</table>

## Ad-hoc reproductions

These are one-off unpublished attempts to reproduce individual studies

<table id="ad_hoc">
			<tbody>
				<tr>
					<td>
						<p>
							Reproduction
						</p>
					</td>
					<td>
						<p>
							Original study
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							https://rdoodles.rbind.io/2019/06/reanalyzing-data-from-human-gut-microbiota-from-autism-spectrum-disorder-promote-behavioral-symptoms-in-mice/
							and
							https://notstatschat.rbind.io/2019/06/16/analysing-the-mouse-autism-data/
						</p>
					</td>
					<td>
						<p>
							Sharon, G. et al. Human Gut Microbiota from Autism Spectrum Disorder Promote Behavioral Symptoms in Mice. Cell 2019, 177 (6), 1600–1618.e17.
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							https://github.com/sean-harrison-bristol/CCR5_replication
						</p>
					</td>
					<td>
						<p>
							Wei, X.; Nielsen, R. CCR5-∆32 Is Deleterious in the Homozygous State in Humans. Nat. Med. 2019 DOI: 10.1038/s41591-019-0459-6. (retracted)
						</p>
					</td>
				</tr>
								<tr>
					<td>
						<p>
							https://github.com/leipzig/placenta
						</p>
					</td>
					<td>
						<p>
							Leiby et al "Lack of detection of a human placenta microbiome in samples from preterm and term deliveries"
https://doi.org/10.1186/s40168-018-0575-4
						</p>
					</td>
				</tr>
				</tbody>
		</table>
			


## Theory papers
<table>
	<tbody>
				<tr>
					<td>
						<p>
							Authors/Date
						</p>
					</td>
					<td>
						<p>
							Title
						</p>
					</td>
					<td>
						<p>
							Field
						</p>
					</td>
					<td>
						<p>
							Type
						</p>
					</td>
				</tr>
		<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1080/09332480.2019.1579573">Ioannidis <meta property="datePublished" content="2005-08-30">2005</a>
						</p>
					</td>
					<td>
						<p>
							<span title="There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.">Why most published research findings are false</span>
						</p>
					<td>
						<p>
							Science
						</p>
					</td>
					<td>
						<p>
							Statistical reproducibility
						</p>
			</td>	
			</td>
				</tr>
						<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1371/journal.pcbi.1000424">Noble <meta property="datePublished" content="2009-07-31">2005</a>
						</p>
					</td>
					<td>
						<p>
							<span title="Most bioinformatics coursework focuses on algorithms, with perhaps some components devoted to learning programming skills and learning how to use existing bioinformatics software. Unfortunately, for students who are preparing for a research career, this type of curriculum fails to address many of the day-to-day organizational challenges associated with performing computational experiments. In practice, the principles behind organizing and documenting computational experiments are often learned on the fly, and this learning is strongly influenced by personal predilections as well as by chance interactions with collaborators or colleagues. The purpose of this article is to describe one good strategy for carrying out computational experiments. I will not describe profound issues such as how to formulate hypotheses, design experiments, or draw conclusions. Rather, I will focus on relatively mundane issues such as organizing files and directories and documenting progress. These issues are important because poor organizational choices can lead to significantly slower research progress. I do not claim that the strategies I outline here are optimal. These are simply the principles and practices that I have developed over 12 years of bioinformatics research, augmented with various suggestions from other researchers with whom I have discussed these issues.">A Quick Guide to Organizing Computational Biology Projects</span>
						</p>
					<td>
						<p>
							Bioinformatics
						</p>
					</td>
					<td>
						<p>
							Best practices
						</p>
			</td>	
			</td>
				</tr>
										<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1371/journal.pcbi.1000424">Sandve et al <meta property="datePublished" content="2013-10-24">2013</a>
						</p>
					</td>
					<td>
						<p>
							<span title="Replication is the cornerstone of a cumulative science [1]. However, new tools and technologies, massive amounts of data, interdisciplinary approaches, and the complexity of the questions being asked are complicating replication efforts, as are increased pressures on scientists to advance their research [2]. As full replication of studies on independently collected data is often not feasible, there has recently been a call for reproducible research as an attainable minimum standard for assessing the value of scientific claims [3]. This requires that papers in experimental science describe the results and provide a sufficiently clear protocol to allow successful repetition and extension of analyses based on original data [4].">Ten Simple Rules for Reproducible Computational Research</span>
						</p>
					<td>
						<p>
							Computational science
						</p>
					</td>
					<td>
						<p>
							Best practices
						</p>
			</td>	
			</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1371/journal.pbio.1002165"><span title="The Economics of Reproducibility in Preclinical Research">Freedman et al <meta property="datePublished" content="2015-06-09">2015</span></a>
						</p>
					</td>
					<td>
						<p>
							<span title="Abstract">The Economics of Reproducibility in Preclinical Research</span>
						</p>
					</td>
					<td>
						<p>
							Preclinical research
						</p>
					</td>
					<td>
						<p>
							Best practices
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.31234/osf.io/jqw35">Yarkoni <meta property="datePublished" content="2019-11-21">2019</a>
						</p>
					</td>
			<td>
						<p>
							<span title="Most theories and hypotheses in psychology are verbal in nature, yet their evaluation overwhelmingly relies on inferential statistical procedures. The validity of the move from qualitative to quantitative analysis depends on the verbal and statistical expressions of a hypothesis being closely aligned—that is, that the two must refer to roughly the same set of hypothetical observations. Here I argue that most inferential statistical tests in psychology fail to meet this basic condition. I demonstrate how foundational assumptions of the 'random effects' model used pervasively in psychology impose far stronger constraints on the generalizability of results than most researchers appreciate. Ignoring these constraints dramatically inflates false positive rates and routinely leads researchers to draw sweeping verbal generalizations that lack any meaningful connection to the statistical quantities they are putatively based on. I argue that the routine failure to consider the generalizability of one's conclusions from a statistical perspective lies at the root of many of psychology's ongoing problems (e.g., the replication crisis), and conclude with a discussion of several potential avenues for improvement.">The Generalizability Crisis</span>
						</p>
					</td>
					<td>
						<p>
							Psychology
						</p>
					</td>
						<td>
						<p>
							Statistical reproducibility
						</p>
			</td>
				</tr>
				<tr>
		<td>
			<p>
				<a href="http://proceedings.mlr.press/v97/bouthillier19a.html">Bouthillier et al <meta property="datePublished" content="2019-06-09">2019</a>
</p>
					</td>
			<td>
						<p>
<span title="The apparent contradiction in the title is a wordplay on the different meanings attributed to the word reproducible across different scientific fields. What we imply is that unreproducible findings can be built upon reproducible methods. Without denying the importance of facilitating the reproduction of methods, we deem important to reassert that reproduction of findings is a fundamental step of the scientific inquiry. We argue that the commendable quest towards easy deterministic reproducibility of methods and numerical results should not have us forget the even more important necessity of ensuring the reproducibility of empirical findings and conclusions by properly accounting for essential sources of variations. We provide experiments to exemplify the brittleness of current common practice in the evaluation of models in the field of deep learning, showing that even if the results could be reproduced, a slightly different experiment would not support the findings. We hope to help clarify the distinction between exploratory and empirical research in the field of deep learning and believe more energy should be devoted to proper empirical research in our community. This work is an attempt to promote the use of more rigorous and diversified methodologies. It is not an attempt to impose a new methodology and it is not a critique on the nature of exploratory research.">Unreproducible Research is Reproducible</span>
</p>
					</td>
					<td>
						<p>
							Machine Learning
						</p>
					</td>
						<td>
						<p>
							Methodology
						</p>
			</td>
				</tr>
				<tr>
					<td>
						<p><a href="https://doi.org/10.1038/s41567-019-0780-5">Milton & Possolo <meta property="datePublished" content="2019-06-09">2019</a>
	</td><td>
						<p><span title="Lack of reproducibility is not necessarily bad news; it may herald new discoveries and signal scientifc progress">Trustworthy data underpin reproducible research</span></p>
	</td>
	<td>
						<p>
							Physics
						</p>
					</td>
						<td>
						<p>
							Scientific philosophy
						</p>
			</td>
				</tr>
                                                                        <tr>
					<td>
						<p>
							<a href="https://doi.org/10.1371/journal.pone.0216125"><span title="Scientific discovery in a model-centric framework: Reproducibility, innovation, and epistemic diversity">Devezer et al <meta property="datePublished" content="2019-05-15">2019</span></a>
						</p>
					</td>
					<td>
						<p>
							<span title="Consistent confirmations obtained independently of each other lend credibility to a scientific result. We refer to results satisfying this consistency as reproducible and assume that reproducibility is a desirable property of scientific discovery. Yet seemingly science also progresses despite irreproducible results, indicating that the relationship between reproducibility and other desirable properties of scientific discovery is not well understood. These properties include early discovery of truth, persistence on truth once it is discovered, and time spent on truth in a long-term scientific inquiry. We build a mathematical model of scientific discovery that presents a viable framework to study its desirable properties including reproducibility. In this framework, we assume that scientists adopt a model-centric approach to discover the true model generating data in a stochastic process of scientific discovery. We analyze the properties of this process using Markov chain theory, Monte Carlo methods, and agent-based modeling. We show that the scientific process may not converge to truth even if scientific results are reproducible and that irreproducible results do not necessarily imply untrue results. The proportion of different research strategies represented in the scientific population, scientists’ choice of methodology, the complexity of truth, and the strength of signal contribute to this counter-intuitive finding. Important insights include that innovative research speeds up the discovery of scientific truth by facilitating the exploration of model space and epistemic diversity optimizes across desirable properties of scientific discovery.">Scientific discovery in a model-centric framework: Reproducibility, innovation, and epistemic diversity</span>
						</p>
					</td>
					<td>
						<p>
							Science
						</p>
					</td>
					<td>
						<p>
							Statistical reproducibility
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://arxiv.org/abs/2002.11626"><span title="A Realistic Guide to Making Data Available Alongside Code to Improve
  Reproducibility">Tierney et al <meta property="datePublished" content="2020-02-06">2020</span></a>
						</p>
					</td>
					<td>
						<p>
							<span title="Data makes science possible. Sharing data improves visibility, and makes the
research process transparent. This increases trust in the work, and allows for
independent reproduction of results. However, a large proportion of data from
published research is often only available to the original authors. Despite the
obvious benefits of sharing data, and scientists' advocating for the importance
of sharing data, most advice on sharing data discusses its broader benefits,
rather than the practical considerations of sharing. This paper provides
practical, actionable advice on how to actually share data alongside research.
The key message is sharing data falls on a continuum, and entering it should
come with minimal barriers.">A Realistic Guide to Making Data Available Alongside Code to Improve
  Reproducibility</span>
						</p>
					</td>
					<td>
						<p>
							Science
						</p>
					</td>
					<td>
						<p>
							Best practices
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://arxiv.org/abs/2003.00898"><span title="The importance of transparency and reproducibility in artificial
  intelligence research">Haibe-Kains et al <meta property="datePublished" content="2020-02-28">2020</span></a>
						</p>
					</td>
					<td>
						<p>
							<span title="In their study, McKinney et al. showed the high potential of artificial
intelligence for breast cancer screening. However, the lack of detailed methods
and computer code undermines its scientific value. We identify obstacles
hindering transparent and reproducible AI research as faced by McKinney et al
and provide solutions with implications for the broader field.">The importance of transparency and reproducibility in artificial
  intelligence research</span>
						</p>
					</td>
					<td>
						<p>
							Artificial Intelligence
						</p>
					</td>
					<td>
						<p>
							Critique
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1371/journal.pbio.3000691"><span title="What is replication?">Nosek & Errington <meta property="datePublished" content="2020-03-27">2020</span></a>
						</p>
					</td>
					<td>
						<p>
							<span title="None">What is replication?</span>
						</p>
					</td>
					<td>
						<p>
							Science
						</p>
					</td>
					<td>
						<p>
							Scientific philosophy
						</p>
					</td>
				</tr>
								<tr>
					<td>
						<p>
							<a href="https://ecoevorxiv.org/h5r6n/"><span title="A Beginner’s Guide to Conducting Reproducible Research">Alston & Rick <meta property="datePublished" content="2020-05-03">2020</span></a>
						</p>
					</td>
					<td>
						<p>
							<span title="Reproducible research is widely acknowledged as an important tool for improving science and reducing harm from the \"replication crisis\", yet research in most fields within biology remains largely irreproducible. In this article, we make the case for why all research should be reproducible, explain why research is often not reproducible, and offer a simple framework that researchers can use to make their research more reproducible. Researchers can increase the reproducibility of their work by improving data management practices, writing more readable code, and increasing use of the many available platforms for sharing data and code. While reproducible research is often associated with a set of advanced tools for sharing data and code, reproducibility is just as much about maintaining work habits that are already widely acknowledged as best practices for research. Increasing reproducibility will increase rigor, trustworthiness, and transparency while benefiting both practitioners of reproducible research and their fellow researchers.">A Beginner’s Guide to Conducting Reproducible Research</span>
						</p>
					</td>
					<td>
						<p>
							Ecology
						</p>
					</td>
					<td>
						<p>
							Best Practices
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.20944/preprints202006.0002.v1"><span title="Realistic and Robust Reproducible Research for Biostatistics">Hejblum et al <meta property="datePublished" content="2020-06-03">2020</span></a>
						</p>
					</td>
					<td>
						<p>
							<span title="&lt;jats:p&gt;The complexity of analysis pipelines in biomedical sciences poses a severe challenge for the transparency and reproducibility of results. Researchers are increasingly incorporating software development technologies and methods into their analyses, but this is a quickly evolving landscape and teams may lack the capabilities to set up their own complex IT infrastructure to aid reproducibility. Basing a reproducible research strategy on readily available solutions with zero or low set-up costs whilst maintaining technological flexibility to incorporate domain-specific software tools is therefore of key importance. We outline a practical approach for robust reproducibility of analysis results. In our examples, we rely exclusively on established open-source tools and free services. Special emphasis is put on the integration of these tools with best practices from software development and free online services for the biostatistics domain.&lt;/jats:p&gt;">Realistic and Robust Reproducible Research for Biostatistics</span>
						</p>
					</td>
					<td>
						<p>
							Biostatistics
						</p>
					</td>
					<td>
						<p>
							Best practices
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1007/s13222-019-00317-8"><span title="A Link is doi.org/10.1007/s13222-019-00317-8 – Reproducibility of Data">Pawlik et al <meta property="datePublished" content="2019-06-13">2019</span></a>
						</p>
					</td>
					<td>
						<p>
							<span title="Abstract">A Link is not Enough – Reproducibility of Data</span>
						</p>
					</td>
					<td>
						<p>
							Databases
						</p>
					</td>
					<td>
						<p>
							Best practices
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1038/s41597-020-0524-5"><span title="COVID-19 pandemic reveals the peril of ignoring metadata standards">Schriml et al <meta property="datePublished" content="2020-06-19">2020</span></a>
						</p>
					</td>
					<td>
						<p>
							<span title="Abstract">COVID-19 pandemic reveals the peril of ignoring metadata standards</span>
						</p>
					</td>
					<td>
						<p>
							Virology
						</p>
					</td>
					<td>
						<p>
							Critique
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://arxiv.org/abs/2007.08708"><span title="Principles for data analysis workflows">Stoudt et al <meta property="datePublished" content="2020-07-17">2020</span></a>
						</p>
					</td>
					<td>
						<p>
							<span title="Traditional data science education often omits training on research
workflows: the process that moves a scientific investigation from raw data to
coherent research question to insightful contribution. In this paper, we
elaborate basic principles of a reproducible data analysis workflow by defining
three phases: the Exploratory, Refinement, and Polishing Phases. Each workflow
phase is roughly centered around the audience to whom research decisions,
methodologies, and results are being immediately communicated. Importantly,
each phase can also give rise to a number of research products beyond
traditional academic publications. Where relevant, we draw analogies between
principles for data-intensive research workflows and established practice in
software development. The guidance provided here is not intended to be a strict
rulebook; rather, the suggestions for practices and tools to advance
reproducible, sound data-intensive analysis may furnish support for both
students and current professionals.">Principles for data analysis workflows</span>
						</p>
					</td>
					<td>
						<p>
							Data science
						</p>
					</td>
					<td>
						<p>
							Best practices
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://arxiv.org/abs/2007.12210"><span title="Reproducible Research: A Retrospective">Peng & Hicks <meta property="datePublished" content="2020-07-23">2020</span></a>
						</p>
					</td>
					<td>
						<p>
							<span title="Rapid advances in computing technology over the past few decades have spurred
two extraordinary phenomena in science: large-scale and high-throughput data
collection coupled with the creation and implementation of complex statistical
algorithms for data analysis. Together, these two phenomena have brought about
tremendous advances in scientific discovery but have also raised two serious
concerns, one relatively new and one quite familiar. The complexity of modern
data analyses raises questions about the reproducibility of the analyses,
meaning the ability of independent analysts to re-create the results claimed by
the original authors using the original data and analysis techniques. While
seemingly a straightforward concept, reproducibility of analyses is typically
thwarted by the lack of availability of the data and computer code that were
used in the analyses. A much more general concern is the replicability of
scientific findings, which concerns the frequency with which scientific claims
are confirmed by completely independent investigations. While the concepts of
reproduciblity and replicability are related, it is worth noting that they are
focused on quite different goals and address different aspects of scientific
progress. In this review, we will discuss the origins of reproducible research,
characterize the current status of reproduciblity in public health research,
and connect reproduciblity to current concerns about replicability of
scientific findings. Finally, we describe a path forward for improving both the
reproducibility and replicability of public health research in the future.">Reproducible Research: A Retrospective</span>
						</p>
					</td>
					<td>
						<p>
							Public health
						</p>
					</td>
					<td>
						<p>
							Review
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1101/2020.06.30.178673"><span title="Streamlining Data-Intensive Biology With Workflow Systems">Reiter et al <meta property="datePublished" content="2020-07-02">2020</span></a>
						</p>
					</td>
					<td>
						<p>
							<span title="&lt;jats:title&gt;Abstract&lt;/jats:title&gt;&lt;jats:p&gt;As the scale of biological data generation has increased, the bottleneck of research has shifted from data generation to analysis. Researchers commonly need to build computational workflows that include multiple analytic tools and require incremental development as experimental insights demand tool and parameter modifications. These workflows can produce hundreds to thousands of intermediate files and results that must be integrated for biological insight. The maturation of data-centric workflow systems that internally manage computational resources, software, and conditional execution of analysis steps are reshaping the landscape of biological data analysis, and empowering researchers to conduct reproducible analyses at scale. Adoption of these tools can facilitate and expedite robust data analysis, but knowledge of these techniques is still lacking. Here, we provide a series of practices and strategies for leveraging workflow systems with structured project, data, and resource management to streamline large-scale biological analysis.&lt;/jats:p&gt;&lt;jats:sec&gt;&lt;jats:title&gt;Author Summary&lt;/jats:title&gt;&lt;jats:p&gt;We present a guide for workflow-enabled biological sequence data analysis, developed through our own teaching, training and analysis projects. We recognize that this is based on our own use cases and experiences, but we hope that our guide will contribute to a larger discussion within the open source and open science communities and lead to more comprehensive resources. Our main goal is to accelerate the research of scientists conducting sequence analyses by introducing them to organized workflow practices that not only benefit their own research but also facilitate open and reproducible science.&lt;/jats:p&gt;&lt;/jats:sec&gt;">Streamlining Data-Intensive Biology With Workflow Systems</span>
						</p>
					</td>
					<td>
						<p>
							Biology
						</p>
					</td>
					<td>
						<p>
							Best practices
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.7554/eLife.58237"><span title="Meta Research: Questionable research practices may have little effect on replicability">Ulrich & Miller <meta property="datePublished" content="2020-09-15">2020</span></a>
						</p>
					</td>
					<td>
						<p>
							<span title="&lt;jats:p&gt;This article examines why many studies fail to replicate statistically significant published results. We address this issue within a general statistical framework that also allows us to include various questionable research practices (QRPs) that are thought to reduce replicability. The analyses indicate that the base rate of true effects is the major factor that determines the replication rate of scientific results. Specifically, for purely statistical reasons, replicability is low in research domains where true effects are rare (e.g., search for effective drugs in pharmacology). This point is under-appreciated in current scientific and media discussions of replicability, which often attribute poor replicability mainly to QRPs.&lt;/jats:p&gt;">Meta Research: Questionable research practices may have little effect on replicability</span>
						</p>
					</td>
					<td>
						<p>
							Science
						</p>
					</td>
					<td>
						<p>
							Statistical reproducibility
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1371/journal.pbio.3000999"><span title="We need to keep a reproducible trace of facts, predictions, and hypotheses from gene to function in the era of big data">Kasif & Roberts <meta property="datePublished" content="2020-11-30">2020</span></a>
						</p>
					</td>
					<td>
						<p>
							<span title="&lt;jats:p&gt;How do we scale biological science to the demand of next generation biology and medicine to keep track of the facts, predictions, and hypotheses? These days, enormous amounts of DNA sequence and other omics data are generated. Since these data contain the blueprint for life, it is imperative that we interpret it accurately. The abundance of DNA is only one part of the challenge. Artificial Intelligence (AI) and network methods routinely build on large screens, single cell technologies, proteomics, and other modalities to infer or predict biological functions and phenotypes associated with proteins, pathways, and organisms. As a first step, how do we systematically trace the provenance of knowledge from experimental ground truth to gene function predictions and annotations? Here, we review the main challenges in tracking the evolution of biological knowledge and propose several specific solutions to provenance and computational tracing of evidence in functional linkage networks.&lt;/jats:p&gt;">We need to keep a reproducible trace of facts, predictions, and hypotheses from gene to function in the era of big data</span>
						</p>
					</td>
					<td>
						<p>
							Functional genomics
						</p>
					</td>
					<td>
						<p>
							Critique
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1093/gigascience/giab015"><span title="A research parasite's perspective on establishing a baseline to avoid errors in secondary analyses">Raman <meta property="datePublished" content="2021-03-12">2021</span></a>
						</p>
					</td>
					<td>
						<p>
							<span title="&lt;jats:title&gt;Abstract&lt;/jats:title&gt;
               &lt;jats:p&gt;To enhance reproducibility in scientific research, more and more datasets are becoming publicly available so that researchers can perform secondary analyses to investigate questions the original scientists had not posited. This increases the return on investment for the NIH and other funding bodies. These datasets, however, are not perfect, and a better understanding of the assumptions that shaped them is required. The 2020 Junior Research Parasite Award recognized our work that showed that the signal-to-noise ratio in a particular dataset had not been investigated, leading to an erroneous conclusion in the original research. In this commentary, I share the process that led to the identification of the problem and hopefully provide useful lessons for other research parasites.&lt;/jats:p&gt;">A research parasite&#x27;s perspective on establishing a baseline to avoid errors in secondary analyses</span>
						</p>
					</td>
					<td>
						<p>
							Science
						</p>
					</td>
					<td>
						<p>
							Best practices
						</p>
					</td>
				</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1098/rsos.201925"><span title="The multiplicity of analysis strategies jeopardizes replicability: lessons learned across disciplines">Hoffmann et al <meta property="datePublished" content="2021-04-21">2021</span></a>
						</p>
					</td>
					<td>
						<p>
							<span title="&lt;jats:p&gt;For a given research question, there are usually a large variety of possible analysis strategies acceptable according to the scientific standards of the field, and there are concerns that this multiplicity of analysis strategies plays an important role in the non-replicability of research findings. Here, we define a general framework on common sources of uncertainty arising in computational analyses that lead to this multiplicity, and apply this framework within an overview of approaches proposed across disciplines to address the issue. Armed with this framework, and a set of recommendations derived therefrom, researchers will be able to recognize strategies applicable to their field and use them to generate findings more likely to be replicated in future studies, ultimately improving the credibility of the scientific process.&lt;/jats:p&gt;">The multiplicity of analysis strategies jeopardizes replicability: lessons learned across disciplines</span>
						</p>
					</td>
					<td>
						<p>
							Science
						</p>
					</td>
					<td>
						<p>
							Critique
						</p>
					</td>
				</tr>
				<!--theory_placeholder-->
	</tbody>
</table>

## Tool reviews
<table>
	<tbody>
		<tr>
			<td>
				<p>
					Authors/Date
				</p>
			</td>
			<td>
				<p>
					Title
				</p>
			</td>
			<td>
				<p>
					Tools
				</p>
			</td>
		</tr>
		<tr>
			<td>
				<p>
					<a href="http://doi.org/10.1109/eScience.2019.00017">Isdahl & Gundersen <meta property="datePublished" content="2019-09-01">2019</a>
				</p>
			</td>
			<td>
				<p>
					<span title="Even machine learning experiments that are fully conducted on computers are not necessarily reproducible. An increasing number of open source and commercial, closed source machine learning platforms are being developed that help address this problem. However, there is no standard for assessing and comparing which features are required to fully support repro-ducibility. We propose a quantitative method that alleviates this problem. Based on the proposed method we assess and compare the current state of the art machine learning platforms for how well they support making empirical results reproducible. Our results show that BEAT and Floydhub have the best support for reproducibility with Codalab and Kaggle as close contenders. The most commonly used machine learning platforms provided by the big tech companies have poor support for reproducibility.">Out-of-the-box Reproducibility: A Survey of Machine Learning Platforms</span>
				</p>
			<td>
				<p>
					MLflow, Polyaxon, StudioML, Kubeflow, CometML, Sagemaker, GCPML, AzureML, Floydhub, BEAT, Codalab, Kaggle
				</p>
			</td>	
		</tr>
				<tr>
					<td>
						<p>
							<a href="https://doi.org/10.1145/3311955">Pimentel et al
							<meta property="datePublished" content="2019-06-19">2019</a>
						</p>
					</td>
					<td>
						<p>
							<span title="Scripts are widely used to design and run scientific experiments. Scripting languages are easy to learn and use, and they allow complex tasks to be specified and executed in fewer steps than with traditional programming languages. However, they also have important limitations for reproducibility and data management. As experiments are iteratively refined, it is challenging to reason about each experiment run (or trial), to keep track of the association between trials and experiment instances as well as the differences across trials, and to connect results to specific input data and parameters. Approaches have been proposed that address these limitations by collecting, managing, and analyzing the provenance of scripts. In this article, we survey the state of the art in provenance for scripts. We have identified the approaches by following an exhaustive protocol of forward and backward literature snowballing. Based on a detailed study, we propose a taxonomy and classify the approaches using this taxonomy.">A Survey on Collecting, Managing, and Analyzing Provenance from Scripts</span>
						</p>
					</td>
					<td>
						<p>
							Astro-Wise, CPL, CXXR, Datatrack, ES3, ESSW, IncPy, Lancet, Magni, noWorkflow, Provenance Curios, pypet, RDataTracker, Sacred, SisGExp, SPADE, StarFlow, Sumatra, Variolite, VCR, versuchung, WISE, YesWorkflow
						</p>
					</td>
				</tr>
                                    		</tr>
                                    				<tr>
                                    					<td>
                                    						<p>
                                    							<a href="https://doi.org/10.1016/j.patter.2021.100322">Leipzig et al
                                    							<meta property="datePublished" content="2021-09-10">2021</a> (<a href="https://github.com/leipzig/metadata-in-rcr">supplemental</a>)
                                    						</p>
                                    					</td>
                                    					<td>
                                    						<p>
                                    							<span title="Reproducible computational research (RCR) is the keystone of the scientific method for in silico analyses, packaging the transformation of raw data to published results. In addition to its role in research integrity, RCR has the capacity to significantly accelerate evaluation and reuse. This potential and wide-support for the FAIR principles have motivated interest in metadata standards supporting RCR. Metadata provides context and provenance to raw data and methods and is essential to both discovery and validation. Despite this shared connection with scientific data, few studies have explicitly described the relationship between metadata and RCR. This article employs a functional content analysis to identify metadata standards that support RCR functions across an analytic stack consisting of input data, tools, notebooks, pipelines, and publications. Our article provides background context, explores gaps, and discovers component trends of embeddedness and methodology weight from which we derive recommendations for future work.">The Role of Metadata in Reproducible Computational Research</span>
                                    						</p>
                                    					</td>
                                    					<td>
                                    						<p>
                                    							CellML, CIF2, DATS, DICOM, EML, FAANG, GBIF, GO, ISO/TC 276, MIAME, NetCDF, OGC, ThermoML, CRAN, Conda, pip setup.cfg, EDAM, CodeMeta, Biotoolsxsd, DOAP, ontosoft, SWO, OBCS, STATO, SDMX, DDI, MEX, MLSchema, MLFlow, Rmd, CWL, CWLProv, RO-Crate, RO, WICUS, OPM, PROV-O, ReproZip, ProvOne, WES, BagIt, BCO, ERC, BEL, DC, JATS, ONIX, MeSH, LCSH, MP, Open PHACTS, SWAN, SPAR, PWO, PAV, Manubot, ReScience, PandocScholar
                                    						</p>
                                    					</td>
                                    				</tr>
								                                    				<tr>
                                    					<td>
                                    						<p>
                                    							<a href="https://doi.org/10.1186/s41073-020-00095-y">Konkol, Markus, Nüst, Daniel, Goulier, Laura
                                    							<meta property="datePublished" content="2020-06-15">2020</a> 
                                    						</p>
                                    					</td>
                                    					<td>
                                    						<p>
                                    							<span title="The trend toward open science increases the pressure on authors to provide access to the source code and data they used to compute the results reported in their scientific papers. Since sharing materials reproducibly is challenging, several projects have developed solutions to support the release of executable analyses alongside articles. We reviewed 11 applications that can assist researchers in adhering to reproducibility principles. The applications were found through a literature search and interactions with the reproducible research community. An application was included in our analysis if it (i) was actively maintained at the time the data for this paper was collected, (ii) supports the publication of executable code and data, (iii) is connected to the scholarly publication process. By investigating the software documentation and published articles, we compared the applications across 19 criteria, such as deployment options and features that support authors in creating and readers in studying executable papers. From the 11 applications, eight allow publishers to self-host the system for free, whereas three provide paid services. Authors can submit an executable analysis using Jupyter Notebooks or R Markdown documents (10 applications support these formats). All approaches provide features to assist readers in studying the materials, e.g., one-click reproducible results or tools for manipulating the analysis parameters. Six applications allow for modifying materials after publication.">Publishing computational research - a review of infrastructures for reproducible and transparent scholarly communication</span>
                                    						</p>
                                    					</td>
                                    					<td>
                                    						<p>
                                    							Authorea, Binder, CodeOcean, eLife RDS, Galaxy Project, Gigantum, Manuscript, o2r, REANA, ReproZip, Whole tale
                                    						</p>
                                    					</td>
                                    				</tr>
				<!--tools_placeholder-->
	</tbody>
</table>

## Courses
- MOOCs
    - [Coursera Reproducible Research](https://www.coursera.org/learn/reproducible-research) - Roger Peng et al JHU. Very popular course.
    - [edX Principles, Statistical and Computational Tools for Reproducible Science](https://www.edx.org/course/principles-statistical-computational-harvardx-ph527x) - John Quackenbush et al Harvard
    - [Reproducible research: methodological principles for transparent science](https://www.fun-mooc.fr/en/courses/reproducible-research-methodological-principles-transparent-scie/) - Beginner level. Note taking, version control, notebooks, reproducible data analysis. Bilingual English/French.
- Online course content
    - [Tools for Reproducible Research](http://kbroman.org/Tools4RR/) - Karl Broman UW, includes resources page
    - [R for Reproducible Scientific Analysis](https://swcarpentry.github.io/r-novice-gapminder/) - Software Carpentry workshop primer using Gapminder data
    - [R-DAVIS](https://gge-ucd.github.io/R-DAVIS/syllabus.html) - Student-developed computer literacy and data course in R
    - [AMIA2019](https://github.com/StatTag/amia-2019-spring-rr/) - Pragmatic RR for Analysis, Dissemination and Publication
    - [PSU-PSY525](https://github.com/psu-psychology/psy-525-reproducible-research-2020) - Transparent, Open, and Reproducible Research Practices in the Social and Behavioral Sciences
    - [Monash-RRR](https://monashdatafluency.github.io/r-rep-res/) - Reproducible Research in R workshop tutorial
    - [OSU-OSRR](https://github.com/cbahlai/OSRR_course) - An open science and reproducible research course targeted at organismal ecologists

## Development Resources
- R
    - [CRAN Task View - Reproducible Research](https://cran.r-project.org/web/views/ReproducibleResearch.html) - packages relevant to RCR in R
    - [liftr](https://liftr.me/) - persistent reproducible reporting through containerized R Markdown documents
    - [repo](https://github.com/franapoli/repo) - provenance framework package
    - [orderly](https://vimc.github.io/orderly/articles/orderly.html) - R package that automates writing reproducible analyses
- Linux-related (polyglot)
    - [Reproducible Builds](https://reproducible-builds.org/) - a set of software development practices that create an independently-verifiable path from source to binary code
- Python
    - [mlf-core](https://mlf-core.com) - Framework to develop GPU deterministic machine learning models with PyTorch, TensorFlow and XGBoost

## Literature tools
- [Scite](https://scite.ai/) - Citation statement AI for discovering and evaluating scientific articles 
- [SciScore](https://www.sciscore.com/) - SciScore methods sections for a variety of rigor criteria and analyzes sentences that contain research resources (antibodies, cell lines, plasmids and software tools) and determines how uniquely identifiable that resource is based off of the provided metadata.
- [Ripeta](https://www.ripeta.com/) - Ripeta quickly scans research manuscripts or articles to identify and record key reproducibility variables, such as data availability, code acknowledgements, and research analysis methods.

## Scientific Data Management Systems
- [DVC](https://dvc.org/) - DVC tracks machine learning models and data sets
- [DataLad](https://www.datalad.org/) - Git-based versioning for data and provenance
- [Overture](https://www.overture.bio/) - Portal, query interface, visualization and schema framework that powers ICGC, KFDC, GDC

## Books
- [Reproducible Research with R and R Studio 2013](https://g.co/kgs/RxcFNm)
- [Implementing Reproducible Research 2014](https://osf.io/s9tya/) - Describes projects: Sumatra, Vistrails, CDE, SOLE, JUMBO, CML, knitr. Content available on OSF.
- [The Practice of Reproducible Research 2017](https://g.co/kgs/jZiMR7) - 31 first person case narratives and intro chapters
- [Dynamic Documents with R and knitr 2015](https://g.co/kgs/dpzkF4)
- [The Turing Way: A Handbook for Reproducible Data Science 2020](https://the-turing-way.netlify.com/introduction/introduction)
- [Reproducibility and Replicability in Science](https://www.nap.edu/catalog/25303/reproducibility-and-replicability-in-science)
- [Reproducibility: Principles, Problems, Practices, and Prospects](https://www.wiley.com/en-ec/Reproducibility:+Principles,+Problems,+Practices,+and+Prospects+-p-9781118864975)

## Databases
- [ReplicationWiki](http://replication.uni-goettingen.de/wiki/index.php) - Database for empirical studies with information about methods, data and software used, availability of replication material and whether replications, corrections or retractions are known. Mostly focused on social sciences.
- [ReproCrawl](https://crawl.reproduciblescience.org/)

## Data Repositories
All these repositories assign Digital Object Identifiers (DOIs) to data
- [DataCite](https://datacite.org) - 12M+ DOIs registered for 46 allocators. Offers APIs and a metadata schema.
- [Data Dryad](https://datadryad.org) - curated, metadata-centric, focused on articles associated with published artices, $120 submission fee (various waivers available)
- [Figshare](https://figshare.com) - 20 GB of free private space, unlimited public space, >2M articles, >5k projects
- [OSF](https://osf.io) - Project-oriented system with access control and integration with popular tools. Unlimited storage for projects, but individual files are limited to 5 gigabytes (GB) each.
- [Zenodo](https://zenodo.org/) - Allows embargoed, restricted access, metadata support. 50GB limit.

## Exemplar Portals
Places to find papers with code or portals to host them
- [Jupyter Gallery](https://github.com/jupyter/jupyter/wiki/A-gallery-of-interesting-Jupyter-Notebooks) - Gallery of interesting Jupyter notebooks
- [Papers With Code](https://paperswithcode.com/) - ML papers with code
- [NARPS](https://github.com/poldrack/narps) - Code related to Neuroimaging Analysis Replication and Prediction Study
- [Codeocean](https://codeocean.com/explore) - A gallery of cloud-based containers with reproducible analyses

## Runnable Papers
Experimental papers that have associated notebooks
### Haibe-Kains lab
<table>
  <tbody>
    <tr>
        <th>
            Publication
        </th>
        <th>
            CodeOcean link
        </th>
    </tr>
    <tr>
        <td>
            <a href="https://doi.org/10.1101/471227">Mer AS et al. Integrative Pharmacogenomics Analysis of Patient Derived Xenografts</a>
        </td>
        <td>
            <a href="https://codeocean.com/capsule/0566399">codeocean.com/capsule/056639</a>
        </td>
    </tr>
    <tr>
        <td>
            <a href="https://doi.org/10.1101/052910">Gendoo, Zon et al. MetaGxData: Clinically Annotated Breast, Ovarian and Pancreatic Cancer Datasets and their Use in Generating a Multi-Cancer Gene Signature</a>
        </td>
        <td>
            <a href="https://codeocean.com/capsule/6438633">codeocean.com/capsule/643863</a>
        </td>
    </tr>
    <tr>
        <td>
            <a href="https://doi.org/10.1093/jamia/ocx062">Yao et al. Tissue specificity of in vitro drug sensitivity</a>
        </td>
        <td>
            <a href="https://codeocean.com/capsule/5502756">codeocean.com/capsule/550275</a>
        </td>
    </tr>
    <tr>
        <td>
            <a href="https://doi.org/10.1038/s41467-017-02136-5">Safikhani Z et al. Gene isoforms as expression-based biomarkers predictive of drug response in vitro</a>
        </td>
        <td>
            <a href="https://codeocean.com/capsule/0002901">codeocean.com/capsule/000290</a>
        </td>
    </tr>
    <tr>
        <td>
            <a href="https://doi.org/10.1158/0008-5472.CAN-17-0096">El-Hachem et al. Integrative cancer pharmacogenomics to infer large-scale drug taxonomy</a>
        </td>
        <td>
            <a href="https://codeocean.com/capsule/4252248">codeocean.com/capsule/425224</a>
        </td>
    </tr>
    <tr>
        <td>
            <a href="https://doi.org/10.12688/f1000research.9611.3">Safikhani Z et al. Revisiting inconsistency in large pharmacogenomic studies</a>
        </td>
        <td>
            <a href="https://codeocean.com/capsule/6276064">codeocean.com/capsule/627606</a>
        </td>
    </tr>
    <tr>
        <td>
            <a href="https://doi.org/10.1101/355602">Sandhu V et al. Meta-analysis of 1,200 transcriptomic profiles identifies a prognostic model for pancreatic ductal adenocarcinoma</a>
        </td>
        <td>
            <a href="https://codeocean.com/capsule/2693620">codeocean.com/capsule/269362</a>
        </td>
    </tr>
    				<tr>
					<td>
							<a href="https://doi.org/10.1093/bib/bbab294">Sharifi-Noghabi et al Drug sensitivity prediction from cell line-based pharmacogenomics data: guidelines for developing machine learning models
					</td>
					<td>
							<a href="https://codeocean.com/capsule/7358839">codeocean.com/capsule/7358839</a>
					</td>
    				</tr>
    				<tr>
					<td>
							<a href="https://doi.org/10.1016/j.phro.2021.04.001">Arrowsmith et al Automated detection of dental artifacts for large-scale radiomic analysis in radiation oncology
					</td>
					<td>
							<a href="https://codeocean.com/capsule/2097894">codeocean.com/capsule/2097894</a>
					</td>
    				</tr>
    				<!--codeocean_placeholder-->
  </tbody>
</table>

### Patcher lab
<table>
                                <tr>
                                                                <th>
                                                                                                Publication
                                                                </th>
                                                                <th>
                                                                                                Github link
                                                                </th>
                                </tr>
                                <tr>
                                                                <td>
                                                                                                <a href="https://www.nature.com/articles/nmeth.4324">Pimental et al 2017. Differential analysis of RNA-seq incorporating quantification uncertainty</a>
                                                                </td>
                                                                <td>
                                                                                                <a href="https://github.com/pachterlab/sleuth_paper_analysis">sleuth_paper_analysis</a>
                                                                </td>
                                </tr>
                                <tr>
                                                                <td>
                                                                                                <a href="https://www.biorxiv.org/content/10.1101/673285v1">Melsted et al 2019. Modular and efficient pre-processing of single-cell RNA-seq</a>
                                                                </td>
                                                                <td>
                                                                                                <a href="https://github.com/pachterlab/MBGBLHGP_2019">MBGBLHGP_2019</a>
                                                                </td>
                                </tr>
                                <tr>
                                                                <td>
                                                                                                <a href="https://doi.org/10.1101/2021.01.22.427844">Chari et al 2021. Whole Animal Multiplexed Single-Cell RNA-Seq Reveals Plasticity of Clytia Medusa Cell Types</a>
                                                                </td>
                                                                <td>
                                                                                                <a href="https://github.com/pachterlab/CWGFLHGCCHAP_2021">CWGFLHGCCHAP_2021</a>
                                                                </td>
                                </tr>
</table>

### Siepel lab
<table>
                                    <tr>
                                                                <td>
                                                                                                <a href="https://bmcbiol.biomedcentral.com/articles/10.1186/s12915-021-00949-x">Blumberg et al 2021. Characterizing RNA stability genome-wide through combined analysis of PRO-seq and RNA-seq data</a>
                                                                </td>
                                                                <td>
                                                                                                <a href="https://codeocean.com/capsule/7351682">https://codeocean.com/capsule/7351682</a>
                                                                </td>
                                </tr>
</table>
    
## Journals
- [ReScience](http://rescience.github.io/) - Journal dedicated to insilico reproductions and tests of robustness, lives on Github.
- [eLife](https://elifesciences.org/for-the-press/eb096af1/elife-launches-executable-research-articles-for-publishing-computationally-reproducible-results) - Executable Research Articles (ERA) inline executable blocks

## Ontologies
- [FAIRsharing](https://fairsharing.org) - standards, databases, and policies
- [BioPortal](https://bioportal.bioontology.org/) - 660 biomedical ontologies 

## Minimal Standards
- [STORMS](https://www.stormsmicrobiome.org/) - Strengthening The Organization and Reporting of Microbiome Studies (STORMS) is a checklist for reporting on human microbiome studies. [Preprint](https://doi.org/10.1101/2020.06.24.167353)

## Organizations
- [ResearchObject.org](http://www.researchobject.org/) - RO specifications and publications
- [BioCompute](https://osf.io/zm97b/) - BCO specs
- [rOpenSci](https://ropensci.org) - Tools, conferences, and education
- [Open Science Framework](https://osf.io) - Open source project management
- [pyOpenSci](https://www.pyopensci.org/) - Promotes open and reproducible research through peer-review of scientific Python packages
- [Replication Network](https://replicationnetwork.com/) - Furthering the practice of replication in economics. Econ replication database.
- [repliCATS project](https://replicats.research.unimelb.edu.au/) - Estimating the replicability of research in the social sciences. [Paper](https://osf.io/preprints/metaarxiv/2pczv/)
- [ReproHack](https://reprohack.github.io/reprohack-hq/) - 1-day reproducibility hackathons held worldwide
- [CODECHECK](https://codecheck.org.uk/) - community for checking executability of scientific preprints and papers
- [CASCaD](https://www.cascad.tech/) - Certification Agency for Scientific Code and Data. Issues reproducibility certificates.
- [Reproducibility for Everyone](https://www.repro4everyone.org/) - Community-led reproducibility workshops

## Awesome Lists
- [Awesome Pipeline](https://github.com/pditommaso/awesome-pipeline) - So many pipelines frameworks
- [Awesome Docker](https://github.com/veggiemonk/awesome-docker) - Everything related to the Docker containerization system
- [Awesome R](https://github.com/qinwf/awesome-R#reproducible-research) - Section on RR tools
- [Awesome Reproducible R](https://github.com/datasnakes/awesome-reproducible-R) - RRR tools
- [Awesome Jupyter](https://github.com/adebar/awesome-jupyter) - Jupyter projects, libraries and resources
- [Awesome Bioinformatics Benchmarks](https://github.com/j-andrews7/Awesome-Bioinformatics-Benchmarks) - Benchmarks are a related aspect of robustness testing
- [Awesome Open Science](https://github.com/ZoranPandovski/awesome-open-science) - Resources, data, tools, and scholarship
- [Awesome Public Datasets](https://github.com/awesomedata/awesome-public-datasets) - A topic-centric list of HQ open datasets
- [Awesome Semantic Web](https://github.com/semantalytics/awesome-semantic-web) - Semantic web and linked data resources.

## Contribute

Contributions welcome! Read the [contribution guidelines](contributing.md) first. You may find my `src/doi2md.py` script useful for quickly generating entries from a DOI.


## License

[![CC0](http://mirrors.creativecommons.org/presskit/buttons/88x31/svg/cc-zero.svg)](https://creativecommons.org/publicdomain/zero/1.0/)

To the extent possible under law, Jeremy Leipzig has waived all copyright and
related or neighboring rights to this work.
